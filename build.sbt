import scala.language.postfixOps

name := "aws"

organization := "com.playgroundcoder.aws"

scalaVersion := "2.13.6"

lazy val dependencies = new {
  // Spark
  val sparkVersion = "3.1.2"
  val sparkCoreProvided = "org.apache.spark" %% "spark-core" % sparkVersion % "provided"
  val sparkSqlProvided = "org.apache.spark" %% "spark-sql" % sparkVersion % "provided"
  val sparkHiveProvided = "org.apache.spark" %% "spark-hive" % sparkVersion % "provided"
  val sparkMlLibProvided = "org.apache.spark" %% "spark-mllib" % sparkVersion % "provided"
  // Command line parser
  val scopt = "com.frugalmechanic" %% "scala-optparse" % "1.1.3"
  // SBT Junit is supported through junit-interface
  val junit = "com.github.sbt" % "junit-interface" % "0.13.2" % Test
  // Scalatest framework
  val scalactic = "org.scalactic" %% "scalactic" % "3.2.9"
  val scalatest = "org.scalatest" %% "scalatest" % "3.2.9" % "test"
  // Generate HTML reports in Scalatest
  val flexmark = "com.vladsch.flexmark" % "flexmark-all" % "0.62.2"
  // Spark testing framework
  val sparkTesting = "com.holdenkarau" %% "spark-testing-base" % "3.0.0_1.0.0" % "test"
  // fastUtils is an efficient type specific collection library
  val fastUtils = "it.unimi.dsi" % "fastutil" % "8.5.4"
  // AWS
  val awsCore = "software.amazon.awssdk" % "aws-core" % "2.16.82"
  val awsS3 = "software.amazon.awssdk" % "s3" % "2.16.82"
  val hadoopAws = "org.apache.hadoop" % "hadoop-aws" % "3.3.0" % Test
  // Json/Gson options
  val playJson = "com.typesafe.play" %% "play-json" % "2.13"
  val gson = "com.google.code.gson" % "gson" % "2.8.7"
}

lazy val testSettings = Seq(
  // Specify Scalatest style traits for uniformity.
  // All scala test classes should extend this style ONLY.
  Test / testOptions += Tests.Argument(
    TestFrameworks.ScalaTest,
    "-y", "org.scalatest.PropSpec",
    "-y", "org.scalatest.FunSpec"
  ),
  Test / testOptions += Tests.Argument(TestFrameworks.JUnit, "-v"),
  Test / testOptions ++= Seq(
    Tests.Argument(TestFrameworks.ScalaTest, "-o"),
    Tests.Argument(TestFrameworks.ScalaTest, "-h", "target/scala-2.13/scalatest-reports")
  ),
  Test / testOptions += Tests.Argument("-oD"), // record test execution time
  Test / parallelExecution := false,
  Test / fork := true
  // Coverage settings
  // Coverage is disabled by default to avoid Scoverage's runtime dependency in application jar.
  // Enable coverage explicitly when needed. `sbt clean coverage test coverageReport`
  // Do not use coverage when publishing jar.
)

lazy val javacSettings = Seq(
  javacOptions ++= Seq(
    "-source", "11",
    "-target", "11"
  )
)

lazy val assemblySettings = Seq(
  // Do not include scala libraries in assembly jar.
  assembly / assemblyOption := (assembly / assemblyOption).value.copy(includeScala = true),
  // assembly name here should match to same generated by publishLocal.
  assemblyJarName := s"${name.value}-${version.value}.jar",
  assembly / assemblyMergeStrategy := {
    case PathList("META-INF", xs @ _*) => MergeStrategy.discard
    case _ => MergeStrategy.first
  },
  // Disable test in assembly
  assembly / test := {}
)

lazy val publishSettings = Seq(
  publishTo := {
    val artifactory = "https://artifactory.com/artifactory/sbt-internalfacing"
    Some("Artifactory Realm".at(artifactory))
  }
)

lazy val root = project
  .in(file("."))
  .enablePlugins(JavaAppPackaging)
  .enablePlugins(UniversalPlugin)
  .settings(
    testSettings,
    assemblySettings,
    publishSettings,
    javacSettings,
    libraryDependencies ++= Seq(
      dependencies.sparkCoreProvided,
      dependencies.sparkSqlProvided,
      dependencies.sparkHiveProvided,
      dependencies.sparkMlLibProvided,
      dependencies.scopt,
      dependencies.junit,
      dependencies.scalactic,
      dependencies.scalatest,
      dependencies.flexmark,
      dependencies.sparkTesting,
      dependencies.fastUtils,
      dependencies.awsCore,
      dependencies.awsS3,
      dependencies.hadoopAws,
      dependencies.playJson,
      dependencies.gson,
    )
  )

Universal / packageName := name.value